{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-atf3gekcgR"
   },
   "source": [
    "# Assessment 1: I can train and deploy a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7wkT17FkmU6"
   },
   "source": [
    "At this point, you've worked through a full deep learning workflow. You've loaded a dataset, trained a model, and deployed your model into a simple application. Validate your learning by attempting to replicate that workflow with a new problem.\n",
    "\n",
    "We've included a dataset which consists of two classes:  \n",
    "\n",
    "1) Face: Contains images which include the face of a whale  \n",
    "2) Not Face: Contains images which do not include the face of a whale.  \n",
    "\n",
    "The dataset is located at ```/dli/data/whale/data/train```.\n",
    "\n",
    "Your challenge is:\n",
    "\n",
    "1) Use [DIGITS](/digits) to train a model to identify *new* whale faces with an accuracy of more than 80%.   \n",
    "\n",
    "2) Deploy your model by modifying and saving the python application [submission.py](../../../../edit/tasks/task-assessment/task/submission.py) to return the word \"whale\" if the image contains a whale's face and \"not whale\" if the image does not.  \n",
    "\n",
    "Resources:\n",
    "\n",
    "1) [Train a model](../../task1/task/Train%20a%20Model.ipynb)  \n",
    "2) [New Data as a goal](../../task2/task/New%20Data%20as%20a%20Goal.ipynb)  \n",
    "3) [Deployment](../../task3/task/Deployment.ipynb)  \n",
    "\n",
    "Suggestions: \n",
    "\n",
    "- Use empty code blocks to find out any informantion necessary to solve this problem: eg: ```!ls [directorypath] prints the files in a given directory``` \n",
    "- Executing the first two cells below will run your python script with test images, the first should return \"whale\" and the second should return \"not whale\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YaaY1Vb3o3mC"
   },
   "source": [
    "Start in [DIGITS](/digits/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0902 13:25:34.286855   256 gpu_memory.cpp:105] GPUMemory::Manager initialized\n",
      "I0902 13:25:34.287413   256 gpu_memory.cpp:107] Total memory: 11996954624, Free: 11801657344, dev_info[0]: total=11996954624 free=11801657344\n",
      "W0902 13:25:34.287468   256 _caffe.cpp:172] DEPRECATION WARNING - deprecated use of Python interface\n",
      "W0902 13:25:34.287578   256 _caffe.cpp:173] Use this instead (with the named \"weights\" parameter):\n",
      "W0902 13:25:34.287595   256 _caffe.cpp:175] Net('/dli/data/digits/20200902-124755-8675/deploy.prototxt', 1, weights='/dli/data/digits/20200902-124755-8675/snapshot_iter_972.caffemodel')\n",
      "I0902 13:25:34.287900   256 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: /dli/data/digits/20200902-124755-8675/deploy.prototxt\n",
      "I0902 13:25:34.287933   256 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.\n",
      "W0902 13:25:34.287945   256 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.\n",
      "I0902 13:25:34.298269   256 net.cpp:79] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TEST\n",
      "  level: 0\n",
      "}\n",
      "layer {\n",
      "  name: \"input\"\n",
      "  type: \"Input\"\n",
      "  top: \"data\"\n",
      "  input_param {\n",
      "    shape {\n",
      "      dim: 1\n",
      "      dim: 3\n",
      "      dim: 227\n",
      "      dim: 227\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 96\n",
      "    kernel_size: 11\n",
      "    stride: 4\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"conv1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm1\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"norm1\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm2\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"norm2\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv4\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv4\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu4\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv4\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv5\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv5\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu5\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"conv5\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool5\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"pool5\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc6\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool5\"\n",
      "  top: \"fc6\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu6\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop6\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc7\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc7\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu7\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop7\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc8\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc8\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"softmax\"\n",
      "  type: \"Softmax\"\n",
      "  bottom: \"fc8\"\n",
      "  top: \"softmax\"\n",
      "}\n",
      "I0902 13:25:34.298703   256 net.cpp:109] Using FLOAT as default forward math type\n",
      "I0902 13:25:34.298727   256 net.cpp:115] Using FLOAT as default backward math type\n",
      "I0902 13:25:34.298739   256 layer_factory.hpp:172] Creating layer 'input' of type 'Input'\n",
      "I0902 13:25:34.298749   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.298768   256 net.cpp:199] Created Layer input (0)\n",
      "I0902 13:25:34.298811   256 net.cpp:541] input -> data\n",
      "I0902 13:25:34.299448   256 net.cpp:259] Setting up input\n",
      "I0902 13:25:34.299479   256 net.cpp:266] TEST Top shape for layer 0 'input' 1 3 227 227 (154587)\n",
      "I0902 13:25:34.299500   256 layer_factory.hpp:172] Creating layer 'conv1' of type 'Convolution'\n",
      "I0902 13:25:34.299518   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.299557   256 net.cpp:199] Created Layer conv1 (1)\n",
      "I0902 13:25:34.299576   256 net.cpp:571] conv1 <- data\n",
      "I0902 13:25:34.299593   256 net.cpp:541] conv1 -> conv1\n",
      "I0902 13:25:34.720480   256 net.cpp:259] Setting up conv1\n",
      "I0902 13:25:34.720530   256 net.cpp:266] TEST Top shape for layer 1 'conv1' 1 96 55 55 (290400)\n",
      "I0902 13:25:34.720556   256 layer_factory.hpp:172] Creating layer 'relu1' of type 'ReLU'\n",
      "I0902 13:25:34.720572   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.720592   256 net.cpp:199] Created Layer relu1 (2)\n",
      "I0902 13:25:34.720604   256 net.cpp:571] relu1 <- conv1\n",
      "I0902 13:25:34.720618   256 net.cpp:526] relu1 -> conv1 (in-place)\n",
      "I0902 13:25:34.720645   256 net.cpp:259] Setting up relu1\n",
      "I0902 13:25:34.720659   256 net.cpp:266] TEST Top shape for layer 2 'relu1' 1 96 55 55 (290400)\n",
      "I0902 13:25:34.720669   256 layer_factory.hpp:172] Creating layer 'norm1' of type 'LRN'\n",
      "I0902 13:25:34.720681   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.720710   256 net.cpp:199] Created Layer norm1 (3)\n",
      "I0902 13:25:34.720721   256 net.cpp:571] norm1 <- conv1\n",
      "I0902 13:25:34.720731   256 net.cpp:541] norm1 -> norm1\n",
      "I0902 13:25:34.720789   256 net.cpp:259] Setting up norm1\n",
      "I0902 13:25:34.720805   256 net.cpp:266] TEST Top shape for layer 3 'norm1' 1 96 55 55 (290400)\n",
      "I0902 13:25:34.720813   256 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'\n",
      "I0902 13:25:34.720824   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.720868   256 net.cpp:199] Created Layer pool1 (4)\n",
      "I0902 13:25:34.720881   256 net.cpp:571] pool1 <- norm1\n",
      "I0902 13:25:34.720893   256 net.cpp:541] pool1 -> pool1\n",
      "I0902 13:25:34.720950   256 net.cpp:259] Setting up pool1\n",
      "I0902 13:25:34.720966   256 net.cpp:266] TEST Top shape for layer 4 'pool1' 1 96 27 27 (69984)\n",
      "I0902 13:25:34.720978   256 layer_factory.hpp:172] Creating layer 'conv2' of type 'Convolution'\n",
      "I0902 13:25:34.720990   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.721014   256 net.cpp:199] Created Layer conv2 (5)\n",
      "I0902 13:25:34.721024   256 net.cpp:571] conv2 <- pool1\n",
      "I0902 13:25:34.721037   256 net.cpp:541] conv2 -> conv2\n",
      "I0902 13:25:34.727082   256 net.cpp:259] Setting up conv2\n",
      "I0902 13:25:34.727109   256 net.cpp:266] TEST Top shape for layer 5 'conv2' 1 256 27 27 (186624)\n",
      "I0902 13:25:34.727129   256 layer_factory.hpp:172] Creating layer 'relu2' of type 'ReLU'\n",
      "I0902 13:25:34.727142   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.727162   256 net.cpp:199] Created Layer relu2 (6)\n",
      "I0902 13:25:34.727178   256 net.cpp:571] relu2 <- conv2\n",
      "I0902 13:25:34.727196   256 net.cpp:526] relu2 -> conv2 (in-place)\n",
      "I0902 13:25:34.727213   256 net.cpp:259] Setting up relu2\n",
      "I0902 13:25:34.727226   256 net.cpp:266] TEST Top shape for layer 6 'relu2' 1 256 27 27 (186624)\n",
      "I0902 13:25:34.727237   256 layer_factory.hpp:172] Creating layer 'norm2' of type 'LRN'\n",
      "I0902 13:25:34.727248   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.727267   256 net.cpp:199] Created Layer norm2 (7)\n",
      "I0902 13:25:34.727277   256 net.cpp:571] norm2 <- conv2\n",
      "I0902 13:25:34.727289   256 net.cpp:541] norm2 -> norm2\n",
      "I0902 13:25:34.727340   256 net.cpp:259] Setting up norm2\n",
      "I0902 13:25:34.727355   256 net.cpp:266] TEST Top shape for layer 7 'norm2' 1 256 27 27 (186624)\n",
      "I0902 13:25:34.727368   256 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'\n",
      "I0902 13:25:34.727375   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.727391   256 net.cpp:199] Created Layer pool2 (8)\n",
      "I0902 13:25:34.727401   256 net.cpp:571] pool2 <- norm2\n",
      "I0902 13:25:34.727408   256 net.cpp:541] pool2 -> pool2\n",
      "I0902 13:25:34.727460   256 net.cpp:259] Setting up pool2\n",
      "I0902 13:25:34.727473   256 net.cpp:266] TEST Top shape for layer 8 'pool2' 1 256 13 13 (43264)\n",
      "I0902 13:25:34.727486   256 layer_factory.hpp:172] Creating layer 'conv3' of type 'Convolution'\n",
      "I0902 13:25:34.727497   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.727517   256 net.cpp:199] Created Layer conv3 (9)\n",
      "I0902 13:25:34.727527   256 net.cpp:571] conv3 <- pool2\n",
      "I0902 13:25:34.727540   256 net.cpp:541] conv3 -> conv3\n",
      "I0902 13:25:34.743178   256 net.cpp:259] Setting up conv3\n",
      "I0902 13:25:34.743204   256 net.cpp:266] TEST Top shape for layer 9 'conv3' 1 384 13 13 (64896)\n",
      "I0902 13:25:34.743227   256 layer_factory.hpp:172] Creating layer 'relu3' of type 'ReLU'\n",
      "I0902 13:25:34.743240   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.743259   256 net.cpp:199] Created Layer relu3 (10)\n",
      "I0902 13:25:34.743270   256 net.cpp:571] relu3 <- conv3\n",
      "I0902 13:25:34.743283   256 net.cpp:526] relu3 -> conv3 (in-place)\n",
      "I0902 13:25:34.743304   256 net.cpp:259] Setting up relu3\n",
      "I0902 13:25:34.743320   256 net.cpp:266] TEST Top shape for layer 10 'relu3' 1 384 13 13 (64896)\n",
      "I0902 13:25:34.743332   256 layer_factory.hpp:172] Creating layer 'conv4' of type 'Convolution'\n",
      "I0902 13:25:34.743348   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.743371   256 net.cpp:199] Created Layer conv4 (11)\n",
      "I0902 13:25:34.743386   256 net.cpp:571] conv4 <- conv3\n",
      "I0902 13:25:34.743396   256 net.cpp:541] conv4 -> conv4\n",
      "I0902 13:25:34.755518   256 net.cpp:259] Setting up conv4\n",
      "I0902 13:25:34.755548   256 net.cpp:266] TEST Top shape for layer 11 'conv4' 1 384 13 13 (64896)\n",
      "I0902 13:25:34.755599   256 layer_factory.hpp:172] Creating layer 'relu4' of type 'ReLU'\n",
      "I0902 13:25:34.755616   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.755630   256 net.cpp:199] Created Layer relu4 (12)\n",
      "I0902 13:25:34.755646   256 net.cpp:571] relu4 <- conv4\n",
      "I0902 13:25:34.755658   256 net.cpp:526] relu4 -> conv4 (in-place)\n",
      "I0902 13:25:34.755681   256 net.cpp:259] Setting up relu4\n",
      "I0902 13:25:34.755698   256 net.cpp:266] TEST Top shape for layer 12 'relu4' 1 384 13 13 (64896)\n",
      "I0902 13:25:34.755710   256 layer_factory.hpp:172] Creating layer 'conv5' of type 'Convolution'\n",
      "I0902 13:25:34.755726   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.755746   256 net.cpp:199] Created Layer conv5 (13)\n",
      "I0902 13:25:34.755759   256 net.cpp:571] conv5 <- conv4\n",
      "I0902 13:25:34.755772   256 net.cpp:541] conv5 -> conv5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0902 13:25:34.763720   256 net.cpp:259] Setting up conv5\n",
      "I0902 13:25:34.763747   256 net.cpp:266] TEST Top shape for layer 13 'conv5' 1 256 13 13 (43264)\n",
      "I0902 13:25:34.763768   256 layer_factory.hpp:172] Creating layer 'relu5' of type 'ReLU'\n",
      "I0902 13:25:34.763788   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.763808   256 net.cpp:199] Created Layer relu5 (14)\n",
      "I0902 13:25:34.763824   256 net.cpp:571] relu5 <- conv5\n",
      "I0902 13:25:34.763836   256 net.cpp:526] relu5 -> conv5 (in-place)\n",
      "I0902 13:25:34.763857   256 net.cpp:259] Setting up relu5\n",
      "I0902 13:25:34.763875   256 net.cpp:266] TEST Top shape for layer 14 'relu5' 1 256 13 13 (43264)\n",
      "I0902 13:25:34.763886   256 layer_factory.hpp:172] Creating layer 'pool5' of type 'Pooling'\n",
      "I0902 13:25:34.763901   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.763917   256 net.cpp:199] Created Layer pool5 (15)\n",
      "I0902 13:25:34.763932   256 net.cpp:571] pool5 <- conv5\n",
      "I0902 13:25:34.763944   256 net.cpp:541] pool5 -> pool5\n",
      "I0902 13:25:34.764008   256 net.cpp:259] Setting up pool5\n",
      "I0902 13:25:34.764027   256 net.cpp:266] TEST Top shape for layer 15 'pool5' 1 256 6 6 (9216)\n",
      "I0902 13:25:34.764039   256 layer_factory.hpp:172] Creating layer 'fc6' of type 'InnerProduct'\n",
      "I0902 13:25:34.764050   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:34.764072   256 net.cpp:199] Created Layer fc6 (16)\n",
      "I0902 13:25:34.764088   256 net.cpp:571] fc6 <- pool5\n",
      "I0902 13:25:34.764101   256 net.cpp:541] fc6 -> fc6\n",
      "I0902 13:25:35.436818   256 net.cpp:259] Setting up fc6\n",
      "I0902 13:25:35.436868   256 net.cpp:266] TEST Top shape for layer 16 'fc6' 1 4096 (4096)\n",
      "I0902 13:25:35.436895   256 layer_factory.hpp:172] Creating layer 'relu6' of type 'ReLU'\n",
      "I0902 13:25:35.436913   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:35.436930   256 net.cpp:199] Created Layer relu6 (17)\n",
      "I0902 13:25:35.436946   256 net.cpp:571] relu6 <- fc6\n",
      "I0902 13:25:35.436961   256 net.cpp:526] relu6 -> fc6 (in-place)\n",
      "I0902 13:25:35.436988   256 net.cpp:259] Setting up relu6\n",
      "I0902 13:25:35.437003   256 net.cpp:266] TEST Top shape for layer 17 'relu6' 1 4096 (4096)\n",
      "I0902 13:25:35.437014   256 layer_factory.hpp:172] Creating layer 'drop6' of type 'Dropout'\n",
      "I0902 13:25:35.437031   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:35.437050   256 net.cpp:199] Created Layer drop6 (18)\n",
      "I0902 13:25:35.437065   256 net.cpp:571] drop6 <- fc6\n",
      "I0902 13:25:35.437075   256 net.cpp:526] drop6 -> fc6 (in-place)\n",
      "I0902 13:25:35.471022   256 net.cpp:259] Setting up drop6\n",
      "I0902 13:25:35.471076   256 net.cpp:266] TEST Top shape for layer 18 'drop6' 1 4096 (4096)\n",
      "I0902 13:25:35.471091   256 layer_factory.hpp:172] Creating layer 'fc7' of type 'InnerProduct'\n",
      "I0902 13:25:35.471107   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:35.471130   256 net.cpp:199] Created Layer fc7 (19)\n",
      "I0902 13:25:35.471179   256 net.cpp:571] fc7 <- fc6\n",
      "I0902 13:25:35.471195   256 net.cpp:541] fc7 -> fc7\n",
      "I0902 13:25:35.770751   256 net.cpp:259] Setting up fc7\n",
      "I0902 13:25:35.770798   256 net.cpp:266] TEST Top shape for layer 19 'fc7' 1 4096 (4096)\n",
      "I0902 13:25:35.770818   256 layer_factory.hpp:172] Creating layer 'relu7' of type 'ReLU'\n",
      "I0902 13:25:35.770833   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:35.770845   256 net.cpp:199] Created Layer relu7 (20)\n",
      "I0902 13:25:35.770859   256 net.cpp:571] relu7 <- fc7\n",
      "I0902 13:25:35.770869   256 net.cpp:526] relu7 -> fc7 (in-place)\n",
      "I0902 13:25:35.770889   256 net.cpp:259] Setting up relu7\n",
      "I0902 13:25:35.770900   256 net.cpp:266] TEST Top shape for layer 20 'relu7' 1 4096 (4096)\n",
      "I0902 13:25:35.770908   256 layer_factory.hpp:172] Creating layer 'drop7' of type 'Dropout'\n",
      "I0902 13:25:35.770920   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:35.770931   256 net.cpp:199] Created Layer drop7 (21)\n",
      "I0902 13:25:35.770942   256 net.cpp:571] drop7 <- fc7\n",
      "I0902 13:25:35.770949   256 net.cpp:526] drop7 -> fc7 (in-place)\n",
      "I0902 13:25:35.804769   256 net.cpp:259] Setting up drop7\n",
      "I0902 13:25:35.804797   256 net.cpp:266] TEST Top shape for layer 21 'drop7' 1 4096 (4096)\n",
      "I0902 13:25:35.804806   256 layer_factory.hpp:172] Creating layer 'fc8' of type 'InnerProduct'\n",
      "I0902 13:25:35.804826   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:35.804850   256 net.cpp:199] Created Layer fc8 (22)\n",
      "I0902 13:25:35.804865   256 net.cpp:571] fc8 <- fc7\n",
      "I0902 13:25:35.804874   256 net.cpp:541] fc8 -> fc8\n",
      "I0902 13:25:35.805131   256 net.cpp:259] Setting up fc8\n",
      "I0902 13:25:35.805147   256 net.cpp:266] TEST Top shape for layer 22 'fc8' 1 2 (2)\n",
      "I0902 13:25:35.805160   256 layer_factory.hpp:172] Creating layer 'softmax' of type 'Softmax'\n",
      "I0902 13:25:35.805171   256 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:25:35.805186   256 net.cpp:199] Created Layer softmax (23)\n",
      "I0902 13:25:35.805197   256 net.cpp:571] softmax <- fc8\n",
      "I0902 13:25:35.805204   256 net.cpp:541] softmax -> softmax\n",
      "I0902 13:25:35.805275   256 net.cpp:259] Setting up softmax\n",
      "I0902 13:25:35.805290   256 net.cpp:266] TEST Top shape for layer 23 'softmax' 1 2 (2)\n",
      "I0902 13:25:35.805296   256 net.cpp:337] softmax does not need backward computation.\n",
      "I0902 13:25:35.805307   256 net.cpp:337] fc8 does not need backward computation.\n",
      "I0902 13:25:35.805313   256 net.cpp:337] drop7 does not need backward computation.\n",
      "I0902 13:25:35.805325   256 net.cpp:337] relu7 does not need backward computation.\n",
      "I0902 13:25:35.805330   256 net.cpp:337] fc7 does not need backward computation.\n",
      "I0902 13:25:35.805335   256 net.cpp:337] drop6 does not need backward computation.\n",
      "I0902 13:25:35.805346   256 net.cpp:337] relu6 does not need backward computation.\n",
      "I0902 13:25:35.805352   256 net.cpp:337] fc6 does not need backward computation.\n",
      "I0902 13:25:35.805358   256 net.cpp:337] pool5 does not need backward computation.\n",
      "I0902 13:25:35.805369   256 net.cpp:337] relu5 does not need backward computation.\n",
      "I0902 13:25:35.805375   256 net.cpp:337] conv5 does not need backward computation.\n",
      "I0902 13:25:35.805382   256 net.cpp:337] relu4 does not need backward computation.\n",
      "I0902 13:25:35.805389   256 net.cpp:337] conv4 does not need backward computation.\n",
      "I0902 13:25:35.805397   256 net.cpp:337] relu3 does not need backward computation.\n",
      "I0902 13:25:35.805402   256 net.cpp:337] conv3 does not need backward computation.\n",
      "I0902 13:25:35.805413   256 net.cpp:337] pool2 does not need backward computation.\n",
      "I0902 13:25:35.805419   256 net.cpp:337] norm2 does not need backward computation.\n",
      "I0902 13:25:35.805433   256 net.cpp:337] relu2 does not need backward computation.\n",
      "I0902 13:25:35.805449   256 net.cpp:337] conv2 does not need backward computation.\n",
      "I0902 13:25:35.805460   256 net.cpp:337] pool1 does not need backward computation.\n",
      "I0902 13:25:35.805498   256 net.cpp:337] norm1 does not need backward computation.\n",
      "I0902 13:25:35.805510   256 net.cpp:337] relu1 does not need backward computation.\n",
      "I0902 13:25:35.805526   256 net.cpp:337] conv1 does not need backward computation.\n",
      "I0902 13:25:35.805541   256 net.cpp:337] input does not need backward computation.\n",
      "I0902 13:25:35.805546   256 net.cpp:379] This network produces output softmax\n",
      "I0902 13:25:35.805575   256 net.cpp:402] Top memory (TEST) required for data: 8315264 diff: 8315264\n",
      "I0902 13:25:35.805586   256 net.cpp:405] Bottom memory (TEST) required for data: 8315256 diff: 8315256\n",
      "I0902 13:25:35.805593   256 net.cpp:408] Shared (in-place) memory (TEST) by data: 2665856 diff: 2665856\n",
      "I0902 13:25:35.805603   256 net.cpp:411] Parameters memory (TEST) required for data: 227505672 diff: 227505672\n",
      "I0902 13:25:35.805609   256 net.cpp:414] Parameters shared memory (TEST) by data: 0 diff: 0\n",
      "I0902 13:25:35.805621   256 net.cpp:420] Network initialization done.\n",
      "I0902 13:25:35.985595   256 net.cpp:1129] Ignoring source layer train-data\n",
      "I0902 13:25:35.985639   256 net.cpp:1137] Copying source layer conv1 Type:Convolution #blobs=2\n",
      "I0902 13:25:35.985764   256 net.cpp:1137] Copying source layer relu1 Type:ReLU #blobs=0\n",
      "I0902 13:25:35.985782   256 net.cpp:1137] Copying source layer norm1 Type:LRN #blobs=0\n",
      "I0902 13:25:35.985795   256 net.cpp:1137] Copying source layer pool1 Type:Pooling #blobs=0\n",
      "I0902 13:25:35.985807   256 net.cpp:1137] Copying source layer conv2 Type:Convolution #blobs=2\n",
      "I0902 13:25:35.986001   256 net.cpp:1137] Copying source layer relu2 Type:ReLU #blobs=0\n",
      "I0902 13:25:35.986019   256 net.cpp:1137] Copying source layer norm2 Type:LRN #blobs=0\n",
      "I0902 13:25:35.986025   256 net.cpp:1137] Copying source layer pool2 Type:Pooling #blobs=0\n",
      "I0902 13:25:35.986032   256 net.cpp:1137] Copying source layer conv3 Type:Convolution #blobs=2\n",
      "I0902 13:25:35.986515   256 net.cpp:1137] Copying source layer relu3 Type:ReLU #blobs=0\n",
      "I0902 13:25:35.986532   256 net.cpp:1137] Copying source layer conv4 Type:Convolution #blobs=2\n",
      "I0902 13:25:35.986898   256 net.cpp:1137] Copying source layer relu4 Type:ReLU #blobs=0\n",
      "I0902 13:25:35.986914   256 net.cpp:1137] Copying source layer conv5 Type:Convolution #blobs=2\n",
      "I0902 13:25:35.987172   256 net.cpp:1137] Copying source layer relu5 Type:ReLU #blobs=0\n",
      "I0902 13:25:35.987190   256 net.cpp:1137] Copying source layer pool5 Type:Pooling #blobs=0\n",
      "I0902 13:25:35.987195   256 net.cpp:1137] Copying source layer fc6 Type:InnerProduct #blobs=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0902 13:25:36.005311   256 net.cpp:1137] Copying source layer relu6 Type:ReLU #blobs=0\n",
      "I0902 13:25:36.005340   256 net.cpp:1137] Copying source layer drop6 Type:Dropout #blobs=0\n",
      "I0902 13:25:36.005345   256 net.cpp:1137] Copying source layer fc7 Type:InnerProduct #blobs=2\n",
      "I0902 13:25:36.013444   256 net.cpp:1137] Copying source layer relu7 Type:ReLU #blobs=0\n",
      "I0902 13:25:36.013476   256 net.cpp:1137] Copying source layer drop7 Type:Dropout #blobs=0\n",
      "I0902 13:25:36.013481   256 net.cpp:1137] Copying source layer fc8 Type:InnerProduct #blobs=2\n",
      "I0902 13:25:36.013523   256 net.cpp:1129] Ignoring source layer loss\n",
      "whale\n"
     ]
    }
   ],
   "source": [
    "!python submission.py '/dli/data/whale/data/train/face/w_1.jpg'  #This should return \"whale\" at the very bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0902 13:27:11.248050   271 gpu_memory.cpp:105] GPUMemory::Manager initialized\n",
      "I0902 13:27:11.248648   271 gpu_memory.cpp:107] Total memory: 11996954624, Free: 11801657344, dev_info[0]: total=11996954624 free=11801657344\n",
      "W0902 13:27:11.248704   271 _caffe.cpp:172] DEPRECATION WARNING - deprecated use of Python interface\n",
      "W0902 13:27:11.248826   271 _caffe.cpp:173] Use this instead (with the named \"weights\" parameter):\n",
      "W0902 13:27:11.248837   271 _caffe.cpp:175] Net('/dli/data/digits/20200902-124755-8675/deploy.prototxt', 1, weights='/dli/data/digits/20200902-124755-8675/snapshot_iter_972.caffemodel')\n",
      "I0902 13:27:11.249176   271 upgrade_proto.cpp:66] Attempting to upgrade input file specified using deprecated input fields: /dli/data/digits/20200902-124755-8675/deploy.prototxt\n",
      "I0902 13:27:11.249209   271 upgrade_proto.cpp:69] Successfully upgraded file specified using deprecated input fields.\n",
      "W0902 13:27:11.249220   271 upgrade_proto.cpp:71] Note that future Caffe releases will only support input layers and not input fields.\n",
      "I0902 13:27:11.260445   271 net.cpp:79] Initializing net from parameters: \n",
      "state {\n",
      "  phase: TEST\n",
      "  level: 0\n",
      "}\n",
      "layer {\n",
      "  name: \"input\"\n",
      "  type: \"Input\"\n",
      "  top: \"data\"\n",
      "  input_param {\n",
      "    shape {\n",
      "      dim: 1\n",
      "      dim: 3\n",
      "      dim: 227\n",
      "      dim: 227\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv1\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"data\"\n",
      "  top: \"conv1\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 96\n",
      "    kernel_size: 11\n",
      "    stride: 4\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu1\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"conv1\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm1\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv1\"\n",
      "  top: \"norm1\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool1\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm1\"\n",
      "  top: \"pool1\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv2\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool1\"\n",
      "  top: \"conv2\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 2\n",
      "    kernel_size: 5\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu2\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"conv2\"\n",
      "}\n",
      "layer {\n",
      "  name: \"norm2\"\n",
      "  type: \"LRN\"\n",
      "  bottom: \"conv2\"\n",
      "  top: \"norm2\"\n",
      "  lrn_param {\n",
      "    local_size: 5\n",
      "    alpha: 0.0001\n",
      "    beta: 0.75\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"pool2\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"norm2\"\n",
      "  top: \"pool2\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"conv3\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"pool2\"\n",
      "  top: \"conv3\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu3\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv3\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv4\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv3\"\n",
      "  top: \"conv4\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 384\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu4\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv4\"\n",
      "}\n",
      "layer {\n",
      "  name: \"conv5\"\n",
      "  type: \"Convolution\"\n",
      "  bottom: \"conv4\"\n",
      "  top: \"conv5\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  convolution_param {\n",
      "    num_output: 256\n",
      "    pad: 1\n",
      "    kernel_size: 3\n",
      "    group: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu5\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"conv5\"\n",
      "}\n",
      "layer {\n",
      "  name: \"pool5\"\n",
      "  type: \"Pooling\"\n",
      "  bottom: \"conv5\"\n",
      "  top: \"pool5\"\n",
      "  pooling_param {\n",
      "    pool: MAX\n",
      "    kernel_size: 3\n",
      "    stride: 2\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc6\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"pool5\"\n",
      "  top: \"fc6\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu6\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop6\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc6\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc7\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc6\"\n",
      "  top: \"fc7\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 4096\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.005\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0.1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"relu7\"\n",
      "  type: \"ReLU\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "}\n",
      "layer {\n",
      "  name: \"drop7\"\n",
      "  type: \"Dropout\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc7\"\n",
      "  dropout_param {\n",
      "    dropout_ratio: 0.5\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"fc8\"\n",
      "  type: \"InnerProduct\"\n",
      "  bottom: \"fc7\"\n",
      "  top: \"fc8\"\n",
      "  param {\n",
      "    lr_mult: 1\n",
      "    decay_mult: 1\n",
      "  }\n",
      "  param {\n",
      "    lr_mult: 2\n",
      "    decay_mult: 0\n",
      "  }\n",
      "  inner_product_param {\n",
      "    num_output: 2\n",
      "    weight_filler {\n",
      "      type: \"gaussian\"\n",
      "      std: 0.01\n",
      "    }\n",
      "    bias_filler {\n",
      "      type: \"constant\"\n",
      "      value: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "layer {\n",
      "  name: \"softmax\"\n",
      "  type: \"Softmax\"\n",
      "  bottom: \"fc8\"\n",
      "  top: \"softmax\"\n",
      "}\n",
      "I0902 13:27:11.260880   271 net.cpp:109] Using FLOAT as default forward math type\n",
      "I0902 13:27:11.260896   271 net.cpp:115] Using FLOAT as default backward math type\n",
      "I0902 13:27:11.260905   271 layer_factory.hpp:172] Creating layer 'input' of type 'Input'\n",
      "I0902 13:27:11.260915   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.260932   271 net.cpp:199] Created Layer input (0)\n",
      "I0902 13:27:11.260948   271 net.cpp:541] input -> data\n",
      "I0902 13:27:11.261588   271 net.cpp:259] Setting up input\n",
      "I0902 13:27:11.261615   271 net.cpp:266] TEST Top shape for layer 0 'input' 1 3 227 227 (154587)\n",
      "I0902 13:27:11.261627   271 layer_factory.hpp:172] Creating layer 'conv1' of type 'Convolution'\n",
      "I0902 13:27:11.261641   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.261667   271 net.cpp:199] Created Layer conv1 (1)\n",
      "I0902 13:27:11.261680   271 net.cpp:571] conv1 <- data\n",
      "I0902 13:27:11.261693   271 net.cpp:541] conv1 -> conv1\n",
      "I0902 13:27:11.674340   271 net.cpp:259] Setting up conv1\n",
      "I0902 13:27:11.674386   271 net.cpp:266] TEST Top shape for layer 1 'conv1' 1 96 55 55 (290400)\n",
      "I0902 13:27:11.674409   271 layer_factory.hpp:172] Creating layer 'relu1' of type 'ReLU'\n",
      "I0902 13:27:11.674427   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.674441   271 net.cpp:199] Created Layer relu1 (2)\n",
      "I0902 13:27:11.674453   271 net.cpp:571] relu1 <- conv1\n",
      "I0902 13:27:11.674461   271 net.cpp:526] relu1 -> conv1 (in-place)\n",
      "I0902 13:27:11.674489   271 net.cpp:259] Setting up relu1\n",
      "I0902 13:27:11.674502   271 net.cpp:266] TEST Top shape for layer 2 'relu1' 1 96 55 55 (290400)\n",
      "I0902 13:27:11.674511   271 layer_factory.hpp:172] Creating layer 'norm1' of type 'LRN'\n",
      "I0902 13:27:11.674522   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.674552   271 net.cpp:199] Created Layer norm1 (3)\n",
      "I0902 13:27:11.674563   271 net.cpp:571] norm1 <- conv1\n",
      "I0902 13:27:11.674576   271 net.cpp:541] norm1 -> norm1\n",
      "I0902 13:27:11.674634   271 net.cpp:259] Setting up norm1\n",
      "I0902 13:27:11.674652   271 net.cpp:266] TEST Top shape for layer 3 'norm1' 1 96 55 55 (290400)\n",
      "I0902 13:27:11.674660   271 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'\n",
      "I0902 13:27:11.674667   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.674715   271 net.cpp:199] Created Layer pool1 (4)\n",
      "I0902 13:27:11.674726   271 net.cpp:571] pool1 <- norm1\n",
      "I0902 13:27:11.674732   271 net.cpp:541] pool1 -> pool1\n",
      "I0902 13:27:11.674789   271 net.cpp:259] Setting up pool1\n",
      "I0902 13:27:11.674805   271 net.cpp:266] TEST Top shape for layer 4 'pool1' 1 96 27 27 (69984)\n",
      "I0902 13:27:11.674814   271 layer_factory.hpp:172] Creating layer 'conv2' of type 'Convolution'\n",
      "I0902 13:27:11.674826   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.674845   271 net.cpp:199] Created Layer conv2 (5)\n",
      "I0902 13:27:11.674855   271 net.cpp:571] conv2 <- pool1\n",
      "I0902 13:27:11.674863   271 net.cpp:541] conv2 -> conv2\n",
      "I0902 13:27:11.680856   271 net.cpp:259] Setting up conv2\n",
      "I0902 13:27:11.680882   271 net.cpp:266] TEST Top shape for layer 5 'conv2' 1 256 27 27 (186624)\n",
      "I0902 13:27:11.680898   271 layer_factory.hpp:172] Creating layer 'relu2' of type 'ReLU'\n",
      "I0902 13:27:11.680912   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.680922   271 net.cpp:199] Created Layer relu2 (6)\n",
      "I0902 13:27:11.680932   271 net.cpp:571] relu2 <- conv2\n",
      "I0902 13:27:11.680939   271 net.cpp:526] relu2 -> conv2 (in-place)\n",
      "I0902 13:27:11.680954   271 net.cpp:259] Setting up relu2\n",
      "I0902 13:27:11.680966   271 net.cpp:266] TEST Top shape for layer 6 'relu2' 1 256 27 27 (186624)\n",
      "I0902 13:27:11.680974   271 layer_factory.hpp:172] Creating layer 'norm2' of type 'LRN'\n",
      "I0902 13:27:11.680984   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.680995   271 net.cpp:199] Created Layer norm2 (7)\n",
      "I0902 13:27:11.681006   271 net.cpp:571] norm2 <- conv2\n",
      "I0902 13:27:11.681013   271 net.cpp:541] norm2 -> norm2\n",
      "I0902 13:27:11.681063   271 net.cpp:259] Setting up norm2\n",
      "I0902 13:27:11.681079   271 net.cpp:266] TEST Top shape for layer 7 'norm2' 1 256 27 27 (186624)\n",
      "I0902 13:27:11.681088   271 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'\n",
      "I0902 13:27:11.681099   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.681123   271 net.cpp:199] Created Layer pool2 (8)\n",
      "I0902 13:27:11.681136   271 net.cpp:571] pool2 <- norm2\n",
      "I0902 13:27:11.681143   271 net.cpp:541] pool2 -> pool2\n",
      "I0902 13:27:11.681192   271 net.cpp:259] Setting up pool2\n",
      "I0902 13:27:11.681208   271 net.cpp:266] TEST Top shape for layer 8 'pool2' 1 256 13 13 (43264)\n",
      "I0902 13:27:11.681218   271 layer_factory.hpp:172] Creating layer 'conv3' of type 'Convolution'\n",
      "I0902 13:27:11.681229   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.681243   271 net.cpp:199] Created Layer conv3 (9)\n",
      "I0902 13:27:11.681253   271 net.cpp:571] conv3 <- pool2\n",
      "I0902 13:27:11.681262   271 net.cpp:541] conv3 -> conv3\n",
      "I0902 13:27:11.696745   271 net.cpp:259] Setting up conv3\n",
      "I0902 13:27:11.696771   271 net.cpp:266] TEST Top shape for layer 9 'conv3' 1 384 13 13 (64896)\n",
      "I0902 13:27:11.696787   271 layer_factory.hpp:172] Creating layer 'relu3' of type 'ReLU'\n",
      "I0902 13:27:11.696800   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.696810   271 net.cpp:199] Created Layer relu3 (10)\n",
      "I0902 13:27:11.696820   271 net.cpp:571] relu3 <- conv3\n",
      "I0902 13:27:11.696827   271 net.cpp:526] relu3 -> conv3 (in-place)\n",
      "I0902 13:27:11.696843   271 net.cpp:259] Setting up relu3\n",
      "I0902 13:27:11.696851   271 net.cpp:266] TEST Top shape for layer 10 'relu3' 1 384 13 13 (64896)\n",
      "I0902 13:27:11.696862   271 layer_factory.hpp:172] Creating layer 'conv4' of type 'Convolution'\n",
      "I0902 13:27:11.696868   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.696887   271 net.cpp:199] Created Layer conv4 (11)\n",
      "I0902 13:27:11.696898   271 net.cpp:571] conv4 <- conv3\n",
      "I0902 13:27:11.696904   271 net.cpp:541] conv4 -> conv4\n",
      "I0902 13:27:11.709046   271 net.cpp:259] Setting up conv4\n",
      "I0902 13:27:11.709072   271 net.cpp:266] TEST Top shape for layer 11 'conv4' 1 384 13 13 (64896)\n",
      "I0902 13:27:11.709125   271 layer_factory.hpp:172] Creating layer 'relu4' of type 'ReLU'\n",
      "I0902 13:27:11.709139   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.709152   271 net.cpp:199] Created Layer relu4 (12)\n",
      "I0902 13:27:11.709164   271 net.cpp:571] relu4 <- conv4\n",
      "I0902 13:27:11.709177   271 net.cpp:526] relu4 -> conv4 (in-place)\n",
      "I0902 13:27:11.709192   271 net.cpp:259] Setting up relu4\n",
      "I0902 13:27:11.709203   271 net.cpp:266] TEST Top shape for layer 12 'relu4' 1 384 13 13 (64896)\n",
      "I0902 13:27:11.709215   271 layer_factory.hpp:172] Creating layer 'conv5' of type 'Convolution'\n",
      "I0902 13:27:11.709226   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.709247   271 net.cpp:199] Created Layer conv5 (13)\n",
      "I0902 13:27:11.709257   271 net.cpp:571] conv5 <- conv4\n",
      "I0902 13:27:11.709270   271 net.cpp:541] conv5 -> conv5\n",
      "I0902 13:27:11.717213   271 net.cpp:259] Setting up conv5\n",
      "I0902 13:27:11.717237   271 net.cpp:266] TEST Top shape for layer 13 'conv5' 1 256 13 13 (43264)\n",
      "I0902 13:27:11.717257   271 layer_factory.hpp:172] Creating layer 'relu5' of type 'ReLU'\n",
      "I0902 13:27:11.717272   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.717279   271 net.cpp:199] Created Layer relu5 (14)\n",
      "I0902 13:27:11.717288   271 net.cpp:571] relu5 <- conv5\n",
      "I0902 13:27:11.717295   271 net.cpp:526] relu5 -> conv5 (in-place)\n",
      "I0902 13:27:11.717310   271 net.cpp:259] Setting up relu5\n",
      "I0902 13:27:11.717319   271 net.cpp:266] TEST Top shape for layer 14 'relu5' 1 256 13 13 (43264)\n",
      "I0902 13:27:11.717330   271 layer_factory.hpp:172] Creating layer 'pool5' of type 'Pooling'\n",
      "I0902 13:27:11.717336   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.717352   271 net.cpp:199] Created Layer pool5 (15)\n",
      "I0902 13:27:11.717363   271 net.cpp:571] pool5 <- conv5\n",
      "I0902 13:27:11.717370   271 net.cpp:541] pool5 -> pool5\n",
      "I0902 13:27:11.717422   271 net.cpp:259] Setting up pool5\n",
      "I0902 13:27:11.717437   271 net.cpp:266] TEST Top shape for layer 15 'pool5' 1 256 6 6 (9216)\n",
      "I0902 13:27:11.717445   271 layer_factory.hpp:172] Creating layer 'fc6' of type 'InnerProduct'\n",
      "I0902 13:27:11.717458   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:11.717468   271 net.cpp:199] Created Layer fc6 (16)\n",
      "I0902 13:27:11.717479   271 net.cpp:571] fc6 <- pool5\n",
      "I0902 13:27:11.717486   271 net.cpp:541] fc6 -> fc6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0902 13:27:12.392154   271 net.cpp:259] Setting up fc6\n",
      "I0902 13:27:12.392202   271 net.cpp:266] TEST Top shape for layer 16 'fc6' 1 4096 (4096)\n",
      "I0902 13:27:12.392227   271 layer_factory.hpp:172] Creating layer 'relu6' of type 'ReLU'\n",
      "I0902 13:27:12.392242   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:12.392254   271 net.cpp:199] Created Layer relu6 (17)\n",
      "I0902 13:27:12.392267   271 net.cpp:571] relu6 <- fc6\n",
      "I0902 13:27:12.392277   271 net.cpp:526] relu6 -> fc6 (in-place)\n",
      "I0902 13:27:12.392294   271 net.cpp:259] Setting up relu6\n",
      "I0902 13:27:12.392305   271 net.cpp:266] TEST Top shape for layer 17 'relu6' 1 4096 (4096)\n",
      "I0902 13:27:12.392313   271 layer_factory.hpp:172] Creating layer 'drop6' of type 'Dropout'\n",
      "I0902 13:27:12.392320   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:12.392333   271 net.cpp:199] Created Layer drop6 (18)\n",
      "I0902 13:27:12.392344   271 net.cpp:571] drop6 <- fc6\n",
      "I0902 13:27:12.392350   271 net.cpp:526] drop6 -> fc6 (in-place)\n",
      "I0902 13:27:12.425989   271 net.cpp:259] Setting up drop6\n",
      "I0902 13:27:12.426014   271 net.cpp:266] TEST Top shape for layer 18 'drop6' 1 4096 (4096)\n",
      "I0902 13:27:12.426026   271 layer_factory.hpp:172] Creating layer 'fc7' of type 'InnerProduct'\n",
      "I0902 13:27:12.426039   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:12.426055   271 net.cpp:199] Created Layer fc7 (19)\n",
      "I0902 13:27:12.426097   271 net.cpp:571] fc7 <- fc6\n",
      "I0902 13:27:12.426111   271 net.cpp:541] fc7 -> fc7\n",
      "I0902 13:27:12.726830   271 net.cpp:259] Setting up fc7\n",
      "I0902 13:27:12.726876   271 net.cpp:266] TEST Top shape for layer 19 'fc7' 1 4096 (4096)\n",
      "I0902 13:27:12.726899   271 layer_factory.hpp:172] Creating layer 'relu7' of type 'ReLU'\n",
      "I0902 13:27:12.726914   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:12.726933   271 net.cpp:199] Created Layer relu7 (20)\n",
      "I0902 13:27:12.726945   271 net.cpp:571] relu7 <- fc7\n",
      "I0902 13:27:12.726958   271 net.cpp:526] relu7 -> fc7 (in-place)\n",
      "I0902 13:27:12.726979   271 net.cpp:259] Setting up relu7\n",
      "I0902 13:27:12.726989   271 net.cpp:266] TEST Top shape for layer 20 'relu7' 1 4096 (4096)\n",
      "I0902 13:27:12.727002   271 layer_factory.hpp:172] Creating layer 'drop7' of type 'Dropout'\n",
      "I0902 13:27:12.727013   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:12.727031   271 net.cpp:199] Created Layer drop7 (21)\n",
      "I0902 13:27:12.727041   271 net.cpp:571] drop7 <- fc7\n",
      "I0902 13:27:12.727053   271 net.cpp:526] drop7 -> fc7 (in-place)\n",
      "I0902 13:27:12.760859   271 net.cpp:259] Setting up drop7\n",
      "I0902 13:27:12.760892   271 net.cpp:266] TEST Top shape for layer 21 'drop7' 1 4096 (4096)\n",
      "I0902 13:27:12.760905   271 layer_factory.hpp:172] Creating layer 'fc8' of type 'InnerProduct'\n",
      "I0902 13:27:12.760917   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:12.760932   271 net.cpp:199] Created Layer fc8 (22)\n",
      "I0902 13:27:12.760944   271 net.cpp:571] fc8 <- fc7\n",
      "I0902 13:27:12.760953   271 net.cpp:541] fc8 -> fc8\n",
      "I0902 13:27:12.761220   271 net.cpp:259] Setting up fc8\n",
      "I0902 13:27:12.761240   271 net.cpp:266] TEST Top shape for layer 22 'fc8' 1 2 (2)\n",
      "I0902 13:27:12.761256   271 layer_factory.hpp:172] Creating layer 'softmax' of type 'Softmax'\n",
      "I0902 13:27:12.761268   271 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT\n",
      "I0902 13:27:12.761288   271 net.cpp:199] Created Layer softmax (23)\n",
      "I0902 13:27:12.761299   271 net.cpp:571] softmax <- fc8\n",
      "I0902 13:27:12.761312   271 net.cpp:541] softmax -> softmax\n",
      "I0902 13:27:12.761382   271 net.cpp:259] Setting up softmax\n",
      "I0902 13:27:12.761397   271 net.cpp:266] TEST Top shape for layer 23 'softmax' 1 2 (2)\n",
      "I0902 13:27:12.761409   271 net.cpp:337] softmax does not need backward computation.\n",
      "I0902 13:27:12.761422   271 net.cpp:337] fc8 does not need backward computation.\n",
      "I0902 13:27:12.761428   271 net.cpp:337] drop7 does not need backward computation.\n",
      "I0902 13:27:12.761440   271 net.cpp:337] relu7 does not need backward computation.\n",
      "I0902 13:27:12.761445   271 net.cpp:337] fc7 does not need backward computation.\n",
      "I0902 13:27:12.761456   271 net.cpp:337] drop6 does not need backward computation.\n",
      "I0902 13:27:12.761461   271 net.cpp:337] relu6 does not need backward computation.\n",
      "I0902 13:27:12.761472   271 net.cpp:337] fc6 does not need backward computation.\n",
      "I0902 13:27:12.761478   271 net.cpp:337] pool5 does not need backward computation.\n",
      "I0902 13:27:12.761484   271 net.cpp:337] relu5 does not need backward computation.\n",
      "I0902 13:27:12.761490   271 net.cpp:337] conv5 does not need backward computation.\n",
      "I0902 13:27:12.761503   271 net.cpp:337] relu4 does not need backward computation.\n",
      "I0902 13:27:12.761508   271 net.cpp:337] conv4 does not need backward computation.\n",
      "I0902 13:27:12.761519   271 net.cpp:337] relu3 does not need backward computation.\n",
      "I0902 13:27:12.761525   271 net.cpp:337] conv3 does not need backward computation.\n",
      "I0902 13:27:12.761536   271 net.cpp:337] pool2 does not need backward computation.\n",
      "I0902 13:27:12.761543   271 net.cpp:337] norm2 does not need backward computation.\n",
      "I0902 13:27:12.761552   271 net.cpp:337] relu2 does not need backward computation.\n",
      "I0902 13:27:12.761559   271 net.cpp:337] conv2 does not need backward computation.\n",
      "I0902 13:27:12.761569   271 net.cpp:337] pool1 does not need backward computation.\n",
      "I0902 13:27:12.761610   271 net.cpp:337] norm1 does not need backward computation.\n",
      "I0902 13:27:12.761621   271 net.cpp:337] relu1 does not need backward computation.\n",
      "I0902 13:27:12.761632   271 net.cpp:337] conv1 does not need backward computation.\n",
      "I0902 13:27:12.761643   271 net.cpp:337] input does not need backward computation.\n",
      "I0902 13:27:12.761654   271 net.cpp:379] This network produces output softmax\n",
      "I0902 13:27:12.761684   271 net.cpp:402] Top memory (TEST) required for data: 8315264 diff: 8315264\n",
      "I0902 13:27:12.761695   271 net.cpp:405] Bottom memory (TEST) required for data: 8315256 diff: 8315256\n",
      "I0902 13:27:12.761701   271 net.cpp:408] Shared (in-place) memory (TEST) by data: 2665856 diff: 2665856\n",
      "I0902 13:27:12.761708   271 net.cpp:411] Parameters memory (TEST) required for data: 227505672 diff: 227505672\n",
      "I0902 13:27:12.761719   271 net.cpp:414] Parameters shared memory (TEST) by data: 0 diff: 0\n",
      "I0902 13:27:12.761730   271 net.cpp:420] Network initialization done.\n",
      "I0902 13:27:12.941974   271 net.cpp:1129] Ignoring source layer train-data\n",
      "I0902 13:27:12.942011   271 net.cpp:1137] Copying source layer conv1 Type:Convolution #blobs=2\n",
      "I0902 13:27:12.942121   271 net.cpp:1137] Copying source layer relu1 Type:ReLU #blobs=0\n",
      "I0902 13:27:12.942137   271 net.cpp:1137] Copying source layer norm1 Type:LRN #blobs=0\n",
      "I0902 13:27:12.942143   271 net.cpp:1137] Copying source layer pool1 Type:Pooling #blobs=0\n",
      "I0902 13:27:12.942153   271 net.cpp:1137] Copying source layer conv2 Type:Convolution #blobs=2\n",
      "I0902 13:27:12.942335   271 net.cpp:1137] Copying source layer relu2 Type:ReLU #blobs=0\n",
      "I0902 13:27:12.942349   271 net.cpp:1137] Copying source layer norm2 Type:LRN #blobs=0\n",
      "I0902 13:27:12.942361   271 net.cpp:1137] Copying source layer pool2 Type:Pooling #blobs=0\n",
      "I0902 13:27:12.942368   271 net.cpp:1137] Copying source layer conv3 Type:Convolution #blobs=2\n",
      "I0902 13:27:12.942814   271 net.cpp:1137] Copying source layer relu3 Type:ReLU #blobs=0\n",
      "I0902 13:27:12.942828   271 net.cpp:1137] Copying source layer conv4 Type:Convolution #blobs=2\n",
      "I0902 13:27:12.943171   271 net.cpp:1137] Copying source layer relu4 Type:ReLU #blobs=0\n",
      "I0902 13:27:12.943186   271 net.cpp:1137] Copying source layer conv5 Type:Convolution #blobs=2\n",
      "I0902 13:27:12.943421   271 net.cpp:1137] Copying source layer relu5 Type:ReLU #blobs=0\n",
      "I0902 13:27:12.943433   271 net.cpp:1137] Copying source layer pool5 Type:Pooling #blobs=0\n",
      "I0902 13:27:12.943439   271 net.cpp:1137] Copying source layer fc6 Type:InnerProduct #blobs=2\n",
      "I0902 13:27:12.960983   271 net.cpp:1137] Copying source layer relu6 Type:ReLU #blobs=0\n",
      "I0902 13:27:12.961017   271 net.cpp:1137] Copying source layer drop6 Type:Dropout #blobs=0\n",
      "I0902 13:27:12.961022   271 net.cpp:1137] Copying source layer fc7 Type:InnerProduct #blobs=2\n",
      "I0902 13:27:12.969089   271 net.cpp:1137] Copying source layer relu7 Type:ReLU #blobs=0\n",
      "I0902 13:27:12.969135   271 net.cpp:1137] Copying source layer drop7 Type:Dropout #blobs=0\n",
      "I0902 13:27:12.969147   271 net.cpp:1137] Copying source layer fc8 Type:InnerProduct #blobs=2\n",
      "I0902 13:27:12.969197   271 net.cpp:1129] Ignoring source layer loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not whale\r\n"
     ]
    }
   ],
   "source": [
    "!python submission.py '/dli/data/whale/data/train/not_face/w_1.jpg'  #This should return \"not whale\" at the very bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Assessment1.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
